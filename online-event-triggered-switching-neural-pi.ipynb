{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of base Neural-PI controllers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A RNN-based Reinforcement Learning Framework for Frequency Control Problem with Stability Guarantee\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from tensorflow.keras import models, layers, optimizers\n",
    "from tensorflow.keras.layers import RNN\n",
    "import tensorflow.keras.backend as K\n",
    "import copy\n",
    "from mat4py import loadmat\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import networkx as nx\n",
    "import scipy\n",
    "import pickle\n",
    "import time\n",
    "from cycler import cycler\n",
    "import matplotlib as mpl\n",
    "from matplotlib import ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "for device in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency Control Porblem Environment\n",
    "class Frequency():\n",
    "    def  __init__(self, Pm,M,current_M,D,F,delta_t,max_action,dim_action,Penalty_action, coef_cost,  incidence_communication,gamma=5):\n",
    "        self.param_gamma = gamma\n",
    "        self.M = current_M\n",
    "        self.D = D\n",
    "        self.F = F\n",
    "        self.Pm = Pm\n",
    "        self.original_M = M\n",
    "        self.max_action = max_action\n",
    "        self.dim_action = dim_action\n",
    "        self.omega_scale = 2*np.pi# this change the unit of omega to Hz\n",
    "        self.state_x = []\n",
    "        self.delta_t = delta_t\n",
    "        self.Penalty_action = Penalty_action\n",
    "        self.state_x_transfer1 = np.vstack((np.hstack((np.identity(dim_action,dtype = np.float32),np.zeros((dim_action,dim_action),dtype = np.float32))),\\\n",
    "                                np.hstack((delta_t*self.omega_scale*np.identity(dim_action,dtype = np.float32),\\\n",
    "                                           np.identity(dim_action,dtype = np.float32)-delta_t*np.diag(D/self.M)))))\n",
    "        self.state_x_transferF = -delta_t*(((self.M**(-1)).reshape(dim_action,1))@np.ones((1,dim_action),dtype = np.float32))*F\n",
    "        self.state_x_transfer2 = np.hstack((np.zeros((dim_action,dim_action),dtype = np.float32),\n",
    "                                            np.identity(dim_action,dtype = np.float32)))        \n",
    "\n",
    "        self.state_x_transfer3 = np.hstack((np.zeros((1,dim_action),dtype = np.float32),\n",
    "                                            delta_t*Pm*(self.M**(-1))))\n",
    "        self.state_x_transfer3_Pm = np.hstack((np.zeros((dim_action,dim_action),dtype = np.float32),\n",
    "                                               delta_t*np.diag((self.M**(-1)))))\n",
    "        self.state_x_transfer4 = np.hstack((np.zeros((dim_action,dim_action),dtype = np.float32),\n",
    "                                            -delta_t*np.diag((self.M**(-1)))))\n",
    "\n",
    "        self.select_add_w = np.vstack((np.zeros((dim_action,1),dtype = np.float32),\\\n",
    "                                        np.ones((dim_action,1),dtype = np.float32)))\n",
    "        self.select_w = np.vstack((np.zeros((dim_action,dim_action),dtype = np.float32),\\\n",
    "                                        np.identity(dim_action,dtype = np.float32)))\n",
    "        self.select_delta = np.vstack((np.identity(dim_action,dtype = np.float32),\\\n",
    "                                        np.zeros((dim_action,dim_action),dtype = np.float32)))\n",
    "        self.diag_c  =  np.diag(coef_cost)\n",
    "        self.incidence_communication = incidence_communication.astype(np.float32)\n",
    "        self.last_action = 0\n",
    "\n",
    "        \n",
    "\n",
    "    def step_PI_CommEdge(self, actionP, actionI,Pm):\n",
    "\n",
    "        cost_action, grad_action  =  self.calc_grad_action(actionI)\n",
    "        action  =  actionP + actionI\n",
    "        dot_s_new  =  self.state_x@self.select_w-self.CommEdgeFeedback(grad_action@self.incidence_communication)@\\\n",
    "                      self.incidence_communication.T@self.diag_c*0.05\n",
    "        self.state_x  =  copy.deepcopy(self.state_x@self.state_x_transfer1\\\n",
    "              + np.sum(np.sin( np.transpose(self.state_x@self.select_delta)@np.ones((1,self.dim_action),dtype = np.float32)-\\\n",
    "                np.ones((self.dim_action,1),dtype = np.float32)@(self.state_x@self.select_delta))*self.state_x_transferF,axis = 1 )\\\n",
    "                      @self.state_x_transfer2\\\n",
    "              + Pm@self.state_x_transfer3_Pm\\\n",
    "                          +action@self.state_x_transfer4)                \n",
    "        self.state_s  =  self.state_s+self.delta_t*dot_s_new/1\n",
    "        loss_action = 0.5*np.sum(np.power(action,2)@self.diag_c)\n",
    "        loss = 1000*np.linalg.norm(self.state_x@self.select_add_w,2)+1000*np.max(np.abs(self.state_x@self.select_add_w))+loss_action\n",
    "        return self.state_s, self.state_x, loss  \n",
    "\n",
    "    def step_PI_CommEdge_WoCost(self, actionP, actionI,Pm):\n",
    "\n",
    "        action  =  actionP + actionI\n",
    "        dot_s_new  =  self.state_x@self.select_w\n",
    "        self.state_x  = copy.deepcopy(self.state_x@self.state_x_transfer1\\\n",
    "              + np.sum(np.sin( np.transpose(self.state_x@self.select_delta)@np.ones((1,self.dim_action),dtype = np.float32)-\\\n",
    "                np.ones((self.dim_action,1),dtype = np.float32)@(self.state_x@self.select_delta))*self.state_x_transferF,axis = 1 )\\\n",
    "                      @self.state_x_transfer2\\\n",
    "              + Pm@self.state_x_transfer3_Pm\\\n",
    "                          +action@self.state_x_transfer4)              \n",
    "        self.state_s  =  self.state_s+self.delta_t*dot_s_new/1\n",
    "        loss  =  self.param_gamma*pow(self.state_x,2)@self.select_add_w \n",
    "        # loss_action = 0.5*np.sum(np.power(action,4)@self.diag_c) + 0.05*np.sum(np.power(action,2)@self.diag_c)\n",
    "        loss_action = 0.5*np.sum(np.power(action,2)@self.diag_c)\n",
    "        loss = loss + 3*loss_action\n",
    "        return self.state_s, self.state_x, loss  \n",
    "\n",
    "    def CommEdgeFeedback(self, x, func = 'linear1'):   \n",
    "        if func == 'linear1':\n",
    "            y = x\n",
    "        return y\n",
    "\n",
    "    def calc_grad_action(self, action):\n",
    "        # return 0.5*(action**4)@self.diag_c + 0.05*(action**2)@self.diag_c, 2*(action**3)@self.diag_c #+  0.1*action@self.diag_c\n",
    "        return 0.5*(action**2)@self.diag_c, action@self.diag_c\n",
    "\n",
    "    def set_state(self, state_input):\n",
    "        self.state_x = state_input\n",
    "        self.state_s  =  np.zeros((1,self.dim_action),dtype = np.float32)\n",
    "    \n",
    "    def switch(self,new_percentage):\n",
    "        self.M = new_percentage*self.original_M\n",
    "        self.state_x_transfer1 = np.vstack((np.hstack((np.identity(self.dim_action,dtype = np.float32),np.zeros((self.dim_action,self.dim_action),dtype = np.float32))),\\\n",
    "                                np.hstack((self.delta_t*self.omega_scale*np.identity(self.dim_action,dtype = np.float32),\\\n",
    "                                           np.identity(self.dim_action,dtype = np.float32)-self.delta_t*np.diag(self.D/self.M)))))\n",
    "        self.state_x_transferF = -self.delta_t*(((self.M**(-1)).reshape(self.dim_action,1))@np.ones((1,self.dim_action),dtype = np.float32))*self.F   \n",
    "\n",
    "        self.state_x_transfer3 = np.hstack((np.zeros((1,self.dim_action),dtype = np.float32),\n",
    "                                            self.delta_t*self.Pm*(self.M**(-1))))\n",
    "        self.state_x_transfer3_Pm = np.hstack((np.zeros((self.dim_action,self.dim_action),dtype = np.float32),\n",
    "                                               self.delta_t*np.diag((self.M**(-1)))))\n",
    "        self.state_x_transfer4 = np.hstack((np.zeros((self.dim_action,self.dim_action),dtype = np.float32),\n",
    "                                            -self.delta_t*np.diag((self.M**(-1)))))\n",
    "\n",
    "    def reset(self):\n",
    "        initial_state1 = np.random.uniform(0.0,0.3,(1,self.dim_action))\n",
    "        initial_state2 = np.random.uniform(-0.03,0.03,(1,self.dim_action))\n",
    "        s_concate = np.hstack((initial_state1,initial_state2)).astype(np.float32)\n",
    "        self.state_x  =  s_concate\n",
    "        self.state_s  =  np.zeros((1,self.dim_action),dtype = np.float32)\n",
    "        return self.state_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation data load from IEEE 39-bus system\n",
    "data = loadmat('data/IEEE_39bus_Kron.mat')\n",
    "\n",
    "K_EN=data['Kron_39bus']['K']\n",
    "K_EN=np.asarray(K_EN, dtype=np.float32)\n",
    "\n",
    "H=data['Kron_39bus']['H']\n",
    "H=np.asarray(H, dtype=np.float32)\n",
    "\n",
    "Damp=data['Kron_39bus']['D']\n",
    "Damp=np.asarray(Damp, dtype=np.float32)\n",
    "\n",
    "omega_R=data['Kron_39bus']['omega_R']\n",
    "\n",
    "A_EN=data['Kron_39bus']['A']\n",
    "A_EN=np.asarray(A_EN, dtype=np.float32)\n",
    "\n",
    "gamma=data['Kron_39bus']['gamma']\n",
    "gamma=np.asarray(gamma, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_action = 10 #dimension of action space\n",
    "dim_state = 2*dim_action #dimension of state space\n",
    "G_comm = nx.random_regular_graph(3, dim_action, seed=4)\n",
    "print(nx.is_connected(G_comm))\n",
    "incidence_comm = scipy.sparse.csr_matrix.toarray(nx.incidence_matrix(G_comm, oriented=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "f = open('data/Comm_power.pckl', 'rb')\n",
    "[G_comm,  incidence_comm]= pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_t=0.01\n",
    "M=H.reshape(dim_action)*2/omega_R*2*np.pi\n",
    "D=np.zeros(dim_action,dtype=np.float32)\n",
    "D[0]=2*590/100\n",
    "D[1:8]=2*865/100\n",
    "D[8:10]=2*911/100\n",
    "D=D/omega_R*2*np.pi\n",
    "F=K_EN\n",
    "Penalty_action=0.01*0.2\n",
    "Pm=np.array([[-0.19983394, -0.25653884, -0.25191885, -0.10242008, -0.34510365,\n",
    "         0.23206371,  0.4404325 ,  0.5896664 ,  0.26257738, -0.36892462]],dtype=np.float32)\n",
    "\n",
    "max_action=np.array([[0.19606592, 0.2190382 , 0.22375287, 0.0975513 , 0.29071101,\n",
    "        0.22091283, 0.38759459, 0.56512538, 0.24151538, 0.29821917]],dtype=np.float32)*5\n",
    "equilibrium_init=np.array([[ -0.05420687, -0.07780334, -0.07351729, -0.05827823, -0.09359571,\n",
    "        -0.02447385, -0.00783582,  0.00259523, -0.0162409 , -0.06477749,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "         0.        ,  0.        ,  0.        ,  0.        ,  0.       ]],dtype=np.float32)\n",
    "coef_cost = np.array([0.94162537, 1.15464676, 0.55487802, 0.71176656, 1.27476119,\n",
    "       1.02529959, 1.2946042 , 1.01800112, 0.99465694, 0.73467357], dtype=np.float32)\n",
    "# np.random.uniform(0.5,1.5,(dim_action))\n",
    "env = Frequency(Pm,M,M,D,F,delta_t,max_action,dim_action,Penalty_action, coef_cost, incidence_comm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN Cell to integrate state transition dynamics\n",
    "global percentage\n",
    "percentage=np.random.choice([0.3,1.0,5.0], 1, p=[1/3, 1/3, 1/3])\n",
    "class MinimalRNNCell(keras.layers.Layer):\n",
    "    def __init__(self, units, action_units_node_p, action_units_node_i, internal_units, env, batchsize, percentage=1.0, fixed= True, env2=None, env3=None, **kwargs):\n",
    "        self.units = units\n",
    "        self.state_size = units\n",
    "        self.action_units_node_p = action_units_node_p\n",
    "        self.action_units_node_i = action_units_node_i  \n",
    "        self.internal_units = internal_units\n",
    "        self.batchsize = batchsize\n",
    "        self.percentage = percentage\n",
    "        self.fixed = fixed\n",
    "        self.delta_t = tf.constant(env.delta_t,dtype = tf.float32)\n",
    "        self.state_x_transfer1 = tf.constant(env.state_x_transfer1,dtype = tf.float32)\n",
    "        self.state_x_transferF = tf.constant(env.state_x_transferF,dtype = tf.float32)\n",
    "        self.state_x_transfer2 = tf.constant(env.state_x_transfer2,dtype = tf.float32)\n",
    "        self.state_x_transfer3 = tf.constant(env.state_x_transfer3,dtype = tf.float32)\n",
    "        self.state_x_transfer4 = tf.constant(env.state_x_transfer4,dtype = tf.float32)\n",
    "        self.state_x_transfer3_Pm = tf.constant(env.state_x_transfer3_Pm,dtype = tf.float32)\n",
    "\n",
    "        if not (env2==None):\n",
    "            self.state_x_transfer1_2 = tf.constant(env2.state_x_transfer1,dtype = tf.float32)\n",
    "            self.state_x_transferF_2 = tf.constant(env2.state_x_transferF,dtype = tf.float32)\n",
    "            self.state_x_transfer2_2 = tf.constant(env2.state_x_transfer2,dtype = tf.float32)\n",
    "            self.state_x_transfer3_2 = tf.constant(env2.state_x_transfer3,dtype = tf.float32)\n",
    "            self.state_x_transfer4_2 = tf.constant(env2.state_x_transfer4,dtype = tf.float32)\n",
    "            self.state_x_transfer3_Pm_2 = tf.constant(env2.state_x_transfer3_Pm,dtype = tf.float32)\n",
    "        if not (env3==None):\n",
    "            self.state_x_transfer1_3 = tf.constant(env3.state_x_transfer1,dtype = tf.float32)\n",
    "            self.state_x_transferF_3 = tf.constant(env3.state_x_transferF,dtype = tf.float32)\n",
    "            self.state_x_transfer2_3 = tf.constant(env3.state_x_transfer2,dtype = tf.float32)\n",
    "            self.state_x_transfer3_3 = tf.constant(env3.state_x_transfer3,dtype = tf.float32)\n",
    "            self.state_x_transfer4_3 = tf.constant(env3.state_x_transfer4,dtype = tf.float32)\n",
    "            self.state_x_transfer3_Pm_3 = tf.constant(env3.state_x_transfer3_Pm,dtype = tf.float32)\n",
    "\n",
    "        self.select_add_w = tf.constant(env.select_add_w,dtype = tf.float32)\n",
    "        self.select_w = tf.constant(env.select_w,dtype = tf.float32)\n",
    "        self.select_delta = tf.constant(env.select_delta,dtype = tf.float32)\n",
    "        self.max_action = tf.constant(env.max_action,dtype = tf.float32)\n",
    "        self.diag_c = tf.constant(env.diag_c, dtype = tf.float32)\n",
    "        self.incidence_communication = tf.constant(env.incidence_communication,dtype = tf.float32)\n",
    "        self.matrix_grad_action =  tf.constant(env.incidence_communication@env.incidence_communication.T@env.diag_c*0.05,dtype = tf.float32)\n",
    "\n",
    "        # Matrix implementation of recover parameters in the Stacked-ReLU Neural Newrok from ancilarily variables\n",
    "        self.w_recover = tf.constant(tf.linalg.band_part(-tf.ones((internal_units,internal_units)),0,1)\\\n",
    "                                        +2*tf.eye(internal_units),dtype = tf.float32)\n",
    "        self.b_recover = tf.constant(tf.linalg.band_part(tf.ones((internal_units,internal_units)),0,-1)\\\n",
    "                                        -tf.eye(internal_units),dtype = tf.float32)\n",
    "\n",
    "        ## helpful matrix for vector implementation of the proportional part\n",
    "        self.Multiply_ones_node_p = tf.tile(tf.ones((action_units_node_p,action_units_node_p),dtype=np.float32)[None], [batchsize, 1, 1]) \n",
    "        self.ones_node_p = tf.ones((action_units_node_p,internal_units),dtype=tf.float32)\n",
    "\n",
    "        ## helpful matrix for vector implementation of the integral part\n",
    "        self.Multiply_ones_node_i = tf.tile(tf.ones((action_units_node_i,action_units_node_i),dtype=np.float32)[None], [batchsize, 1, 1]) \n",
    "        self.ones_node_i = tf.ones((action_units_node_i,internal_units),dtype=tf.float32)\n",
    "        super(MinimalRNNCell, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Intermediate parameters to enforce constraints of the monotone neural network \n",
    "        ## for proportional part\n",
    "        self.w_plus_temp0_node_p =  self.add_weight(\n",
    "            shape=(self.action_units_node_p,self.internal_units),\n",
    "            initializer=tf.constant_initializer(1.0),\n",
    "            trainable=True,\n",
    "            name='w_plus_temp_node_p')\n",
    "\n",
    "        self.b_plus_temp0_node_p = self.add_weight(\n",
    "            shape=(self.action_units_node_p,self.internal_units),\n",
    "            initializer=tf.keras.initializers.RandomUniform(minval=0, maxval=0.2),\n",
    "            trainable=True,\n",
    "            constraint=tf.keras.constraints.MaxNorm(0.2),\n",
    "            name='b_plus_temp_node_p')\n",
    "        self.w_minus_temp0_node_p =  self.add_weight(\n",
    "            shape=(self.action_units_node_p,self.internal_units),\n",
    "            initializer=tf.constant_initializer(1.0),\n",
    "            trainable=True,\n",
    "            name='w_minus_temp_node_p')\n",
    "\n",
    "        self.b_minus_temp0_node_p = self.add_weight(\n",
    "            shape=(self.action_units_node_p,self.internal_units),\n",
    "            initializer=tf.keras.initializers.RandomUniform(minval=0, maxval=0.2),\n",
    "            trainable=True,\n",
    "            constraint=tf.keras.constraints.MaxNorm(0.2),\n",
    "            name='b_minus_temp_node_p')\n",
    "        \n",
    "        \n",
    "        ## for integral part\n",
    "        self.k_integral = self.add_weight(\n",
    "            shape=(1,),\n",
    "            initializer=tf.constant_initializer(6.0), #2.64\n",
    "            trainable=True,\n",
    "            name='k_integral'\n",
    "        )\n",
    "        \n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, states):  \n",
    "        # percentage=np.random.choice([0.2,0.6,1.0], 1, p=[0.3, 0.4, 0.3])  \n",
    "        self.percentage = percentage\n",
    "        prev_state = states[0] # Size: Batch_size*state_size  300*30\n",
    "        prev_state_x = prev_state[:,0:self.action_units_node_p*2]\n",
    "        prev_state_s =  prev_state[:,self.action_units_node_p*2:self.action_units_node_p*3]\n",
    "        #########   control action parameterization ##########\n",
    "        # Monotone neural network by stacking relu function\n",
    "        ## for proportional part\n",
    "        w_plus_temp_node_p = tf.math.square(self.w_plus_temp0_node_p)\n",
    "        b_plus_temp_node_p = tf.math.square(self.b_plus_temp0_node_p)\n",
    "        w_minus_temp_node_p = tf.math.square(self.w_minus_temp0_node_p)\n",
    "        b_minus_temp_node_p = tf.math.square(self.b_minus_temp0_node_p)\n",
    "        w_plus_node_p = K.dot(w_plus_temp_node_p,self.w_recover)\n",
    "        b_plus_node_p = K.dot(-b_plus_temp_node_p,self.b_recover)\n",
    "        w_minus_node_p = K.dot(-w_minus_temp_node_p,self.w_recover)\n",
    "        b_minus_node_p = K.dot(-b_minus_temp_node_p,self.b_recover)\n",
    "\n",
    "        nonlinear_plus_node_p = K.sum(K.relu(K.dot(tf.linalg.diag( K.dot(prev_state_x,self.select_w)),self.ones_node_p)+b_plus_node_p)\\\n",
    "                        *w_plus_node_p,axis=2)  \n",
    "        nonlinear_minus_node_p = K.sum(K.relu(-K.dot(tf.linalg.diag(K.dot(prev_state_x,self.select_w)),self.ones_node_p)+b_minus_node_p)\\\n",
    "                        *w_minus_node_p,axis=2)  \n",
    "\n",
    "        action_nonconstrain0_node_p = (nonlinear_plus_node_p+nonlinear_minus_node_p)\n",
    "        action_node_p = action_nonconstrain0_node_p\n",
    "\n",
    "\n",
    "        ## for integral part\n",
    "        k_int = tf.math.abs(self.k_integral)\n",
    "        # # K_int_matrix = tf.linalg.diag(k_int)\n",
    "        # # action_node_i = K.dot(prev_state_s, K_int_matrix)\n",
    "        \n",
    "        action_node_i = prev_state_s * k_int\n",
    "\n",
    "        ## PI control law            \n",
    "        action_nonconstrain = action_node_i + action_node_p\n",
    "        ## make the action between higher and lower bound\n",
    "        action =  self.max_action-K.relu(self.max_action-action_nonconstrain)+K.relu(-self.max_action-action_nonconstrain)\n",
    "\n",
    "\n",
    "        #########   calculate state transition functions  #######      \n",
    "        # calculate state on s\n",
    "        # grad_action = tf.math.pow(action_node_i,3)@self.diag_c  # quardratic\n",
    "        grad_action = action_node_i@self.diag_c\n",
    "        dot_s = K.dot(prev_state_x,self.select_w)- K.dot(grad_action, self.matrix_grad_action)\n",
    "\n",
    "        # integrate in the state transition dynamics\n",
    "        if self.percentage == 0.3:\n",
    "            if not self.fixed:\n",
    "                self.percentage=np.random.choice([0.3,1.0], 1, p=[0.8, 0.2])\n",
    "            new_state_x = K.dot(prev_state_x, self.state_x_transfer1)+\\\n",
    "                K.dot(K.sum(K.sin( K.dot(tf.linalg.diag(K.dot(prev_state_x, self.select_delta)),tf.ones((dim_action,dim_action),dtype = np.float32))-\\\n",
    "                                    tf.matmul(self.Multiply_ones_node_p,tf.linalg.diag(K.dot(prev_state_x, self.select_delta))))\\\n",
    "                            *self.state_x_transferF,axis = 2 )\\\n",
    "                                        ,self.state_x_transfer2)\\\n",
    "                                + self.state_x_transfer3+K.dot(action,self.state_x_transfer4)\\\n",
    "                                + inputs@self.state_x_transfer3_Pm\n",
    "        elif self.percentage == 1.0: #0.6\n",
    "            if not self.fixed:\n",
    "                self.percentage=np.random.choice([0.3,1.0,5.0], 1, p=[0.1, 0.8, 0.1])\n",
    "            new_state_x = K.dot(prev_state_x, self.state_x_transfer1_2)+\\\n",
    "                K.dot(K.sum(K.sin( K.dot(tf.linalg.diag(K.dot(prev_state_x, self.select_delta)),tf.ones((dim_action,dim_action),dtype = np.float32))-\\\n",
    "                                    tf.matmul(self.Multiply_ones_node_p,tf.linalg.diag(K.dot(prev_state_x, self.select_delta))))\\\n",
    "                            *self.state_x_transferF_2,axis = 2 )\\\n",
    "                                        ,self.state_x_transfer2_2)\\\n",
    "                                + self.state_x_transfer3_2+K.dot(action,self.state_x_transfer4_2)\\\n",
    "                                + inputs@self.state_x_transfer3_Pm_2\n",
    "        else:   # percentage == 1.0:\n",
    "            if not self.fixed:\n",
    "                self.percentage=np.random.choice([1.0,5.0], 1, p=[0.2, 0.8])\n",
    "            new_state_x = K.dot(prev_state_x, self.state_x_transfer1_3)+\\\n",
    "                K.dot(K.sum(K.sin( K.dot(tf.linalg.diag(K.dot(prev_state_x, self.select_delta)),tf.ones((dim_action,dim_action),dtype = np.float32))-\\\n",
    "                                    tf.matmul(self.Multiply_ones_node_p,tf.linalg.diag(K.dot(prev_state_x, self.select_delta))))\\\n",
    "                            *self.state_x_transferF_3,axis = 2 )\\\n",
    "                                        ,self.state_x_transfer2_3)\\\n",
    "                                + self.state_x_transfer3_3+K.dot(action,self.state_x_transfer4_3)\\\n",
    "                                + inputs@self.state_x_transfer3_Pm_3\n",
    "\n",
    "\n",
    "        loss0 = K.dot(K.pow(new_state_x,2),self.select_add_w)\n",
    "        frequency = K.dot(new_state_x,self.select_w)\n",
    "        new_state_s = prev_state_s + self.delta_t*dot_s\n",
    "        next_state = tf.concat([new_state_x,  new_state_s], axis = 1)        \n",
    "        return [loss0, frequency, action_node_p, action_node_i, action], [next_state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train controller with inertia mode 5\n",
    "env = Frequency(Pm, M, M * 0.3,D,F,delta_t,max_action*10,dim_action,Penalty_action*3, coef_cost, incidence_comm, gamma=5)\n",
    "env2 = Frequency(Pm, M, M * 1.0,D,F,delta_t,max_action*10,dim_action,Penalty_action*3, coef_cost, incidence_comm, gamma=5)\n",
    "env3 = Frequency(Pm, M, M * 5.0,D,F,delta_t,max_action*10,dim_action,Penalty_action*3, coef_cost, incidence_comm, gamma=5)\n",
    "start = time.time()\n",
    "\n",
    "episodes  = 500 # total number of iterations to update weights\n",
    "units = dim_action*3 #dimension of each state\n",
    "internal_units = 20 #number of neurons in each hidden layer\n",
    "T = 300  #Total time period\n",
    "Batch_num = 300 # number of batch in each episodes\n",
    "\n",
    "\n",
    "# RNN initialization\n",
    "cell_5 = MinimalRNNCell(units,dim_action, dim_action, internal_units,env,Batch_num, percentage=5.0, fixed=True,env2=env2,env3=env3)\n",
    "layer_5 = RNN(cell_5,return_sequences = True,stateful = True)\n",
    "input_5 = tf.keras.Input(batch_shape = (Batch_num,T,dim_action))\n",
    "outputs = layer_5((input_5))\n",
    "model_mono_5 = tf.keras.models.Model([input_5], outputs)\n",
    "model_mono_5.compile(optimizer = 'adam', loss = 'mse', metrics = ['accuracy'])\n",
    "\n",
    "x0 = np.ones((Batch_num,T,dim_action))\n",
    "y0 = model_mono_5(x0)\n",
    "global_step = tf.Variable(0, trainable = False)\n",
    "learning_rate_initial = 0.05\n",
    "decayed_lr  = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    learning_rate_initial, 50, 0.8, staircase = True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = decayed_lr)\n",
    "# optimizer = tf.keras.optimizers.AdamW(learning_rate = decayed_lr)\n",
    "\n",
    "# Randomly generated traning set \n",
    "PrintUpdate = 1\n",
    "num_gen_step = 3 # pickup three generators of have a step load change\n",
    "Percent_step_change = 1\n",
    "range_step_change = 3 # upper and lower bound of the step load change\n",
    "Loss_record_mono = []\n",
    "\n",
    "# Training\n",
    "for i in range(0,episodes):\n",
    "    initial_state = np.zeros((Batch_num,dim_action*2))+equilibrium_init\n",
    "    Pm_change = np.zeros((Batch_num,T,dim_action))\n",
    "    percentage = 5.0\n",
    "    \n",
    "    for gen_interupt in range(0, num_gen_step):\n",
    "        idx_gen_deviation = np.random.randint(0, dim_action, Batch_num*Percent_step_change)\n",
    "        idx_batch_deviation = np.random.randint(0, Batch_num, Batch_num*Percent_step_change)\n",
    "        slot_start_deviation = np.random.randint(0, T, Batch_num*Percent_step_change)  \n",
    "        step_change = np.random.uniform(-1,1,(Batch_num*Percent_step_change))*range_step_change    \n",
    "        for t_interupt in range(0,T):\n",
    "            Pm_change[idx_batch_deviation,t_interupt, idx_gen_deviation]\\\n",
    "                            = (slot_start_deviation>= t_interupt)*step_change\n",
    "    \n",
    "    layer_5.reset_states( np.hstack((initial_state, np.zeros((Batch_num,dim_action)))))\n",
    "    with tf.GradientTape(persistent = True) as tape:\n",
    "        [loss0,frequency,action0,actions, action] = model_mono_5(Pm_change)\n",
    "        loss_action = 0.5 * K.sum(K.pow(action,2)@env.diag_c)/Batch_num/T \n",
    "        loss_freq = 10*K.sum(tf.norm(frequency, axis=2))/Batch_num/T  + 10*K.sum(K.max(K.abs(frequency),axis = 2))/Batch_num/T \n",
    "        loss = 1*loss_action + 100*loss_freq\n",
    "\n",
    "    grads = tape.gradient(loss, model_mono_5.variables)\n",
    "    optimizer.apply_gradients(zip(grads, model_mono_5.variables))  \n",
    "    Loss_record_mono.append(loss)\n",
    "    if i % (PrintUpdate) ==  0:\n",
    "        print('episode',i, 'Loss',loss)\n",
    "        print('episode',i, 'Loss_frequency',loss_freq)\n",
    "\n",
    "end = time.time()\n",
    "print(end - start) \n",
    "model_mono_5.save_weights('checkpoints/M5/weights_RNN_M5.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Neural-PI-0.3\n",
    "env = Frequency(Pm, M, M*0.3,D,F,delta_t,max_action*10,dim_action,Penalty_action*3, coef_cost, incidence_comm, gamma=5)\n",
    "env2 = Frequency(Pm, M, M*1.0,D,F,delta_t,max_action*10,dim_action,Penalty_action*3, coef_cost, incidence_comm, gamma=5)\n",
    "env3 = Frequency(Pm, M, M*5.0,D,F,delta_t,max_action*10,dim_action,Penalty_action*3, coef_cost, incidence_comm, gamma=5)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "episodes  = 500 # total number of iterations to update weights\n",
    "units = dim_action*3 #dimension of each state\n",
    "internal_units = 20 # number of neurons in each hidden layer\n",
    "T = 300  #Total time period\n",
    "Batch_num = 300 # number of batch in each episodes\n",
    "\n",
    "\n",
    "# RNN initialization\n",
    "cell_03 = MinimalRNNCell(units,dim_action, dim_action, internal_units,env,Batch_num, percentage=0.3, fixed=True,env2=env2,env3=env3)\n",
    "layer_03 = RNN(cell_03,return_sequences = True,stateful = True)\n",
    "input_03 = tf.keras.Input(batch_shape = (Batch_num,T,dim_action))\n",
    "outputs = layer_03((input_03))\n",
    "model_mono_03 = tf.keras.models.Model([input_03], outputs)\n",
    "model_mono_03.compile(optimizer = 'adam', loss = 'mse', metrics = ['accuracy'])\n",
    "# model_mono_05.load_weights('/home/jason/Documents/research/switching/Neural-PI/checkpoints/M02/weights_RNN_M02.ckpt')\n",
    "\n",
    "x0 = np.ones((Batch_num,T,dim_action))\n",
    "y0 = model_mono_03(x0)\n",
    "global_step = tf.Variable(0, trainable = False)\n",
    "learning_rate_initial = 0.05\n",
    "decayed_lr  = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    learning_rate_initial, 50, 0.7, staircase = True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = decayed_lr)\n",
    "\n",
    "# Randomly generated traning set \n",
    "PrintUpdate = 1\n",
    "num_gen_step = 3 # pickup three generators of have a step load change\n",
    "Percent_step_change = 1\n",
    "range_step_change = 3 # upper and lower bound of the step load change\n",
    "Loss_record_mono = []\n",
    "\n",
    "# Training\n",
    "for i in range(0,episodes):\n",
    "    initial_state = np.zeros((Batch_num,dim_action*2))+equilibrium_init\n",
    "    Pm_change = np.zeros((Batch_num,T,dim_action))\n",
    "    percentage=0.3\n",
    "    \n",
    "    for gen_interupt in range(0, num_gen_step):\n",
    "        idx_gen_deviation = np.random.randint(0, dim_action, Batch_num*Percent_step_change)\n",
    "        idx_batch_deviation = np.random.randint(0, Batch_num, Batch_num*Percent_step_change)\n",
    "        slot_start_deviation = np.random.randint(0, T, Batch_num*Percent_step_change)  \n",
    "        step_change = np.random.uniform(-1,1,(Batch_num*Percent_step_change))*range_step_change    \n",
    "        for t_interupt in range(0,T):\n",
    "            Pm_change[idx_batch_deviation,t_interupt, idx_gen_deviation]\\\n",
    "                            = (slot_start_deviation>= t_interupt)*step_change\n",
    "    \n",
    "    layer_03.reset_states( np.hstack((initial_state, np.zeros((Batch_num,dim_action)))))\n",
    "    with tf.GradientTape(persistent = True) as tape:\n",
    "        [loss0,frequency,action0,actions, action] = model_mono_03(Pm_change)\n",
    "        loss_action = 0.5*K.sum(K.pow(action,2)@env.diag_c)/Batch_num/T \n",
    "        loss_freq = 10*K.sum(tf.norm(frequency, axis=2))/Batch_num/T + 10*K.sum(K.max(K.abs(frequency),axis = 2))/Batch_num/T \n",
    "        loss = 1.1*loss_action + 100*loss_freq\n",
    "\n",
    "    grads = tape.gradient(loss, model_mono_03.variables)\n",
    "    optimizer.apply_gradients(zip(grads, model_mono_03.variables))  \n",
    "    Loss_record_mono.append(loss)\n",
    "    if i % (PrintUpdate) ==  0:\n",
    "        print('episode',i, 'Loss',loss)\n",
    "        print('episode',i, 'Loss_frequency',loss_freq)\n",
    "\n",
    "end = time.time()\n",
    "print(end - start) \n",
    "model_mono_03.save_weights('checkpoints/M02/weights_RNN_M02.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Neural-PI-1\n",
    "env = Frequency(Pm, M, M*0.3,D,F,delta_t,max_action*10,dim_action,Penalty_action*3, coef_cost, incidence_comm, gamma=5)\n",
    "env2 = Frequency(Pm, M, M*1.0,D,F,delta_t,max_action*10,dim_action,Penalty_action*3, coef_cost, incidence_comm, gamma=5)\n",
    "env3 = Frequency(Pm, M, M*5.0,D,F,delta_t,max_action*10,dim_action,Penalty_action*3, coef_cost, incidence_comm, gamma=5)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "episodes  = 500 # total number of iterations to update weights\n",
    "units = dim_action*3 #dimension of each state\n",
    "internal_units = 20 # number of neurons in each hidden layer\n",
    "T = 300  #Total time period\n",
    "Batch_num = 300 # number of batch in each episodes\n",
    "\n",
    "\n",
    "# RNN initialization\n",
    "cell_1 = MinimalRNNCell(units,dim_action, dim_action, internal_units,env,Batch_num, percentage=1.0, fixed=True,env2=env2,env3=env3)\n",
    "layer_1 = RNN(cell_1,return_sequences = True,stateful = True)\n",
    "input_1 = tf.keras.Input(batch_shape = (Batch_num,T,dim_action))\n",
    "outputs = layer_1((input_1))\n",
    "model_mono_1 = tf.keras.models.Model([input_1], outputs)\n",
    "model_mono_1.compile(optimizer = 'adam', loss = 'mse', metrics = ['accuracy'])\n",
    "\n",
    "x0 = np.ones((Batch_num,T,dim_action))\n",
    "y0 = model_mono_1(x0)\n",
    "global_step = tf.Variable(0, trainable = False)\n",
    "learning_rate_initial = 0.05\n",
    "decayed_lr  = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    learning_rate_initial, 50, 0.70, staircase = True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = decayed_lr)\n",
    "\n",
    "# Randomly generated traning set \n",
    "PrintUpdate = 1\n",
    "num_gen_step = 3 # pickup three generators of have a step load change\n",
    "Percent_step_change = 1\n",
    "range_step_change = 3 # upper and lower bound of the step load change\n",
    "Loss_record_mono = []\n",
    "\n",
    "# Training\n",
    "for i in range(0,episodes):\n",
    "    # env.switch(new_percentage)\n",
    "    initial_state = np.zeros((Batch_num,dim_action*2))+equilibrium_init\n",
    "    Pm_change = np.zeros((Batch_num,T,dim_action))\n",
    "    # percentage=np.random.choice([0.3,1.0], 1, p=[0.01, 0.99])\n",
    "    percentage=1.0\n",
    "    \n",
    "    for gen_interupt in range(0, num_gen_step):\n",
    "        idx_gen_deviation = np.random.randint(0, dim_action, Batch_num*Percent_step_change)\n",
    "        idx_batch_deviation = np.random.randint(0, Batch_num, Batch_num*Percent_step_change)\n",
    "        slot_start_deviation = np.random.randint(0, T, Batch_num*Percent_step_change)  \n",
    "        step_change = np.random.uniform(-1,1,(Batch_num*Percent_step_change))*range_step_change    \n",
    "        for t_interupt in range(0,T):\n",
    "            Pm_change[idx_batch_deviation,t_interupt, idx_gen_deviation]\\\n",
    "                            = (slot_start_deviation>= t_interupt)*step_change\n",
    "    \n",
    "    layer_1.reset_states( np.hstack((initial_state, np.zeros((Batch_num,dim_action)))))\n",
    "    with tf.GradientTape(persistent = True) as tape:\n",
    "        [loss0,frequency,action0,actions, action] = model_mono_1(Pm_change)\n",
    "        loss_action = 0.5*K.sum(K.pow(action,2)@env.diag_c)/Batch_num /T \n",
    "        # loss_freq = 1*K.sum(K.max(K.abs(frequency),axis = 1))/Batch_num + 0.05*K.sum(K.abs(frequency))/Batch_num\n",
    "        loss_freq = 10*K.sum(tf.norm(frequency, axis=2))/Batch_num /T + 10*K.sum(K.max(K.abs(frequency),axis = 2))/Batch_num/T \n",
    "        loss = 1*loss_action + 100*loss_freq\n",
    "\n",
    "    grads = tape.gradient(loss, model_mono_1.variables)\n",
    "    optimizer.apply_gradients(zip(grads, model_mono_1.variables))  \n",
    "    Loss_record_mono.append(loss)\n",
    "    if i % (PrintUpdate) ==  0:\n",
    "        print('episode',i, 'Loss',loss)\n",
    "        print('episode',i, 'Loss_frequency',loss_freq)\n",
    "\n",
    "end = time.time()\n",
    "print(end - start) \n",
    "model_mono_1.save_weights('checkpoints/M1/weights_RNN_M1.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Action_P(state, model, cell, env):\n",
    "    \n",
    "    w_plus=K.dot(tf.math.square(model.variables[0]),cell.w_recover)\n",
    "    b_plus=K.dot(-tf.math.square(model.variables[1]),cell.b_recover)\n",
    "    w_minus=K.dot(-tf.math.square(model.variables[2]),cell.w_recover)\n",
    "    b_minus=K.dot(-tf.math.square(model.variables[3]),cell.b_recover)\n",
    "    nonlinear_plus=K.sum(K.relu(K.dot(tf.linalg.diag(state@env.select_w),cell.ones_node_p)+b_plus)\\\n",
    "                    *w_plus,axis=2)  \n",
    "    nonlinear_minus=K.sum(K.relu(-K.dot(tf.linalg.diag(state@env.select_w),cell.ones_node_p)+b_minus)\\\n",
    "                    *w_minus,axis=2)  \n",
    "    action_nonconstrain0= nonlinear_plus+nonlinear_minus\n",
    "\n",
    "\n",
    "    return action_nonconstrain0\n",
    "\n",
    "def Action_I(state_s, model, cell, env):\n",
    "    # state_s = tf.convert_to_tensor(state_s)\n",
    "    # k_int = tf.math.abs(model.variables[4].numpy())\n",
    "    k_int = 9 # this is a rounded average of the trained k for the integral controller, which is shared by different base controllers\n",
    "    action_node_i = k_int*state_s\n",
    "    return action_node_i\n",
    "\n",
    "def Action(state_x, state_s, model, cell, env):\n",
    "    action_nonconstrain0 = Action_P(state_x, model, cell, env)\n",
    "    action_nonconstrain1 = Action_I(state_s, model, cell, env)\n",
    "    action_nonconstrain =  action_nonconstrain0 + action_nonconstrain1\n",
    "    action=env.max_action-K.relu(env.max_action-action_nonconstrain)+K.relu(-env.max_action-action_nonconstrain)\n",
    "    return action, action_nonconstrain0, action_nonconstrain1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Frequency(Pm, M, M*0.3, D,F,delta_t,max_action,dim_action,Penalty_action*3, coef_cost, incidence_comm, gamma=5)\n",
    "env2 = Frequency(Pm, M, M*1.0, D,F,delta_t,max_action,dim_action,Penalty_action*3, coef_cost, incidence_comm, gamma=5)\n",
    "env3 = Frequency(Pm, M, M*5.0, D,F,delta_t,max_action,dim_action,Penalty_action*3, coef_cost, incidence_comm, gamma=5)\n",
    "\n",
    "episodes  = 1500 # total number of iterations to update weights\n",
    "units = dim_action*3 #dimension of each state\n",
    "internal_units = 20 # number of neurons in each hidden layer\n",
    "T = 400  #Total time period\n",
    "Batch_num = 300 # number of batch in each episodes\n",
    "\n",
    "cell_03 = MinimalRNNCell(units,dim_action, dim_action, internal_units,env,Batch_num, percentage=0.3, fixed=True,env2=env2,env3=env3)\n",
    "layer_03 = RNN(cell_03,return_sequences = True,stateful = True)\n",
    "input_03 = tf.keras.Input(batch_shape = (Batch_num,T,dim_action))\n",
    "outputs = layer_03((input_03))\n",
    "model_mono_03 = tf.keras.models.Model([input_03], outputs)\n",
    "model_mono_03.compile(optimizer = 'adam', loss = 'mse', metrics = ['accuracy'])\n",
    "model_mono_03.load_weights('checkpoints/M02/weights_RNN_M02.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_1 = MinimalRNNCell(units,dim_action, dim_action, internal_units,env,Batch_num, percentage=1.0, fixed=True,env2=env2,env3=env3)\n",
    "layer_1 = RNN(cell_1,return_sequences = True,stateful = True)\n",
    "input_1 = tf.keras.Input(batch_shape = (Batch_num,T,dim_action))\n",
    "outputs = layer_1((input_1))\n",
    "model_mono_1 = tf.keras.models.Model([input_1], outputs)\n",
    "model_mono_1.compile(optimizer = 'adam', loss = 'mse', metrics = ['accuracy'])\n",
    "model_mono_1.load_weights('/checkpoints/M1/weights_RNN_M1.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_5 = MinimalRNNCell(units,dim_action, dim_action, internal_units,env,Batch_num, percentage=0.3, fixed=True,env2=env2,env3=env3)\n",
    "layer_5 = RNN(cell_5,return_sequences = True,stateful = True)\n",
    "input_5 = tf.keras.Input(batch_shape = (Batch_num,T,dim_action))\n",
    "outputs = layer_5((input_5))\n",
    "model_mono_5 = tf.keras.models.Model([input_5], outputs)\n",
    "model_mono_5.compile(optimizer = 'adam', loss = 'mse', metrics = ['accuracy'])\n",
    "model_mono_5.load_weights('checkpoints/M5/weights_RNN_M5.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test in time-invariant inertias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dist = np.random.uniform(-1,1,(100,(Pm_init).shape[1])) # generate disturbance for 100 trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record for fixed single environment\n",
    "controller_set = [layer_03, layer_1, layer_5]\n",
    "cell_set = [cell_03, cell_1, cell_5]\n",
    "env = Frequency(Pm,M, M*5,D,F,delta_t,max_action, dim_action,Penalty_action*3, coef_cost, incidence_comm,gamma=5)\n",
    "SimulationLength = 300\n",
    "\n",
    "id = 2#controller id\n",
    "loss_list=[] \n",
    "action_loss_list = []\n",
    "freq_loss_list = []\n",
    "for j in range(100):\n",
    "    Record_u = []\n",
    "    record_freq = []\n",
    "    init_state=equilibrium_init\n",
    "\n",
    "    # init_state=env.reset()\n",
    "    x=init_state.copy().astype(np.float32)\n",
    "    env.set_state(x)\n",
    "    Loss_RNN=0\n",
    "    Loss_RNN_discounted=0\n",
    "    Pm_init=Pm.copy()\n",
    "    Pm1=Pm_init.copy().astype(np.float32)\n",
    "    Pm2=(Pm_init.copy()).astype(np.float32)\n",
    "\n",
    "    Pm2 = Pm2 + dist[j]\n",
    "    action_s = np.zeros((1,dim_action),dtype=np.float32)\n",
    "    record_grad_ui = []\n",
    "    Trajectory_s = []\n",
    "    Trajectory_s.append(np.squeeze(env.state_s))\n",
    "    for i in range(SimulationLength):\n",
    "        if i<int(10) :\n",
    "            Pm_change=Pm1.copy()\n",
    "        if i>=int(10) and i<int(400):\n",
    "            Pm_change=Pm2.copy()\n",
    "\n",
    "        u, up, ui = Action(x, action_s, controller_set[id], cell_set[id], env)\n",
    "\n",
    "        action_s, next_x, r= env.step_PI_CommEdge(up,ui,Pm_change)\n",
    "        freq = np.dot(next_x, env.select_w)\n",
    "        record_freq.append(freq)\n",
    "\n",
    "        Loss_RNN_discounted+=r\n",
    "        Loss_RNN+=r\n",
    "        x=next_x\n",
    "        Record_u.append(np.squeeze(u))\n",
    "        \n",
    "\n",
    "    record_freq = np.squeeze(np.asarray(record_freq))\n",
    "    action_array = np.asarray(Record_u)\n",
    "    loss_action = 0.5*np.mean(np.sum(np.power(action_array,2)@env.diag_c,axis=1))\n",
    "    # loss_freq = np.sum(np.max(np.abs(record_freq)))/record_freq.shape[0] + 2*np.sum(np.abs(record_freq))/record_freq.shape[0]\n",
    "    loss_freq = 10*np.mean(np.linalg.norm(record_freq,axis=1)) + 10*np.mean(np.max(np.abs(record_freq),axis=1))\n",
    "    loss = 1*loss_action + 100*loss_freq\n",
    "    loss_list.append(loss.copy())\n",
    "    action_loss_list.append(loss_action.copy())\n",
    "    freq_loss_list.append(loss_freq.copy()*100)\n",
    "print(np.mean(loss_list),np.std(loss_list))\n",
    "print(np.mean(action_loss_list),np.std(action_loss_list))\n",
    "print(np.mean(freq_loss_list),np.std(freq_loss_list))\n",
    "\n",
    "# controller 0.3    1.0   5.0 \n",
    "# 0.3 M:     22.50 (8.25),  335.07 (2.84), 488.07 (3.03)\n",
    "# 1.0 M:     22.74 (8.37),  11.23 (2.40), 13.10 （4.47）\n",
    "# 5.0 M:     23.81 (8.35),  11.24（2.34）, 10.52 (2.06) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long horizon, two disturbances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controller_set = [layer_05, layer_1, layer_03]\n",
    "cell_set = [cell_05, cell_1, cell_03]\n",
    "id = 0\n",
    "\n",
    "action_loss_list = []\n",
    "freq_loss_list = []\n",
    "total_loss_list = []\n",
    "transient_action_cost_list = []\n",
    "transient_freq_cost_list = []\n",
    "transient_cost_list = []\n",
    "\n",
    "for i in range(100):\n",
    "    percentage_plot = np.random.choice([0.3,1.0,5.0], 1, p=[0.1, 0.45, 0.45])[0]\n",
    "    env = Frequency(Pm, M, M*percentage_plot,D,F,delta_t,max_action, dim_action,Penalty_action*3, coef_cost, incidence_comm,gamma=5)\n",
    "    Trajectory_RNN=[] \n",
    "    Record_u=[]\n",
    "    Record_up=[]\n",
    "    Record_ui=[]\n",
    "    record_freq = []\n",
    "    record_freq_norm =[]\n",
    "    record_grad_ui = []\n",
    "    Trajectory_s = []\n",
    "    mode_traj = []\n",
    "\n",
    "    init_state=equilibrium_init\n",
    "    transient_list = []\n",
    "    transient_action_list = []\n",
    "\n",
    "    # init_state=env.reset()\n",
    "    x=init_state.copy().astype(np.float32)\n",
    "    env.set_state(x)\n",
    "    Trajectory_RNN.append(x)\n",
    "    SimulationLength=2000\n",
    "    Pm_init=Pm.copy()\n",
    "    action_s = np.zeros((1,dim_action),dtype=np.float32)\n",
    "    transient_start = False\n",
    "    \n",
    "    mode_traj.append(percentage_plot)\n",
    "    Pm_change=Pm_init.copy()\n",
    "    for j in range(SimulationLength):\n",
    "        if j==10:\n",
    "            Pm_change=Pm_init.copy() + np.random.uniform(-1,1,((Pm_init).shape[1]))\n",
    "            transient_start = True\n",
    "            transient_time = 0\n",
    "        elif j ==700:\n",
    "            Pm_change=Pm_init.copy() + np.random.uniform(-1,1,((Pm_init).shape[1]))\n",
    "            transient_start = True\n",
    "            transient_time = 0\n",
    "\n",
    "        if (j+1)%500 == 0:\n",
    "            if percentage_plot == 0.3:\n",
    "                percentage_plot = np.random.choice([0.3,1.0], 1, p=[0.5, 0.5])[0]\n",
    "            elif percentage_plot == 1.0:\n",
    "                percentage_plot = np.random.choice([0.3,1.0,5.0], 1, p=[0.3, 0.4, 0.3])[0]\n",
    "            else:\n",
    "                percentage_plot = np.random.choice([1.0,5.0], 1, p=[0.5, 0.5])[0]\n",
    "            env.switch(percentage_plot)\n",
    "            mode_traj.append(percentage_plot)\n",
    "\n",
    "        u, up, ui = Action(x, action_s, controller_set[id], cell_set[id], env)\n",
    "\n",
    "        action_s, next_x, r= env.step_PI_CommEdge(up,ui,Pm_change)\n",
    "        freq = np.dot(next_x, env.select_w)\n",
    "        if transient_start:\n",
    "            transient_list.append(freq)\n",
    "            transient_action_list.append(u)\n",
    "            transient_time +=1\n",
    "            if transient_time==300:\n",
    "                transient_start = False\n",
    "        record_freq.append(freq)\n",
    "        record_freq_norm.append(np.linalg.norm(freq))\n",
    "\n",
    "        x=next_x\n",
    "        Trajectory_RNN.append(x)\n",
    "        Record_u.append(np.squeeze(u))\n",
    "        Record_up.append(np.squeeze(up))\n",
    "        Record_ui.append(np.squeeze(ui))\n",
    "        record_grad_ui.append(np.squeeze(env.calc_grad_action(ui)[1]))\n",
    "        Trajectory_s.append(np.squeeze(env.state_s))\n",
    "\n",
    "    record_freq = np.squeeze(np.asarray(record_freq))\n",
    "    action_array = np.asarray(Record_u)\n",
    "\n",
    "    loss_action = 0.5*np.mean(np.sum(np.power(action_array,2)@env.diag_c,axis=1))\n",
    "    # loss_freq = np.sum(np.max(np.abs(record_freq)))/record_freq.shape[0] + 2*np.sum(np.abs(record_freq))/record_freq.shape[0]\n",
    "    loss_freq = 10*np.mean(np.linalg.norm(record_freq,axis=1))+ 10*np.mean(np.max(np.abs(record_freq),axis=1))\n",
    "    loss = 1*loss_action + 100*loss_freq\n",
    "    action_loss_list.append(loss_action)\n",
    "    freq_loss_list.append(100*loss_freq)\n",
    "    total_loss_list.append(loss)\n",
    "    \n",
    "    transient_freq = 10*np.mean(np.linalg.norm(np.squeeze(transient_list),axis=1)) + 10*np.mean(np.max(np.abs(transient_list),axis=1))\n",
    "    transient_act = np.squeeze(np.asarray(transient_action_list))\n",
    "    transient_act_cost = 0.5*np.mean(np.sum(np.power(transient_act,2)@env.diag_c,axis=1))\n",
    "    trans_cost = transient_act_cost + 100*transient_freq\n",
    "    transient_cost_list.append(trans_cost)\n",
    "    transient_action_cost_list.append(transient_act_cost)\n",
    "    transient_freq_cost_list.append(transient_freq*100)\n",
    "\n",
    "    # print(loss,loss_action,5*loss_freq)\n",
    "print('total cost:', np.mean(total_loss_list), np.std(total_loss_list))\n",
    "print('freq: ',np.mean(freq_loss_list),np.std(freq_loss_list))\n",
    "print('action: ',np.mean(action_loss_list),np.std(action_loss_list))\n",
    "\n",
    "print('***********************Transient**************************')\n",
    "print('transient total: ',np.mean(transient_cost_list),np.std(transient_cost_list))\n",
    "print('transient freq: ', np.mean(transient_freq_cost_list),np.std(transient_freq_cost_list))\n",
    "print('transient action: ', np.mean(transient_action_cost_list), np.std(transient_action_cost_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exp-ISS with event-triggering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class exp3_ISS_algm():\n",
    "    def __init__(self, tau, controller_set, cell_set,eta):\n",
    "        self.controller_set = controller_set\n",
    "        self.cell_set = cell_set\n",
    "        self.tau = tau\n",
    "        self.greedy = 0.00\n",
    "        self.controller_num = len(self.controller_set)\n",
    "        self.p = np.ones(self.controller_num)/self.controller_num\n",
    "        self.G = np.zeros(self.controller_num)\n",
    "        self.g = np.zeros(self.controller_num)\n",
    "        self.g_hat = np.zeros(self.controller_num)\n",
    "        self.controller_idx_set = [i for i in range(self.controller_num)]\n",
    "        self.batch_cost_list = []\n",
    "        self.freq_before_switch = 0\n",
    "        self.current_controller_id = self.sample()\n",
    "        self.eta = eta\n",
    "\n",
    "        self.detect_step = 0\n",
    "        self.detect_start=False\n",
    "        self.detect_finished = 0\n",
    "        self.detect_version_record = False\n",
    "        \n",
    "        self.old_controller_id = -1\n",
    "    def current_p(self):\n",
    "        return self.p\n",
    "                \n",
    "        \n",
    "    def update(self,cost_batch,controller_idx,eta=0.5):\n",
    "        epsilon = 0.001\n",
    "        for cidx in self.controller_idx_set:\n",
    "            if cidx == controller_idx:\n",
    "                self.g_hat[cidx] = cost_batch/(self.p[cidx]+epsilon)\n",
    "            self.G[cidx] = self.G[cidx] + self.g_hat[cidx]\n",
    "        tmp = 0\n",
    "        for cidx in self.controller_idx_set:\n",
    "            tmp += np.exp(-self.eta*self.G[cidx])\n",
    "        for cidx in self.controller_idx_set:\n",
    "            self.p[cidx] = (np.exp(-self.eta*self.G[cidx]))/ (tmp)\n",
    "        self.g_hat = np.zeros(self.controller_num)\n",
    "        return self.p\n",
    "    \n",
    "    def detect(self, cost, freq, step):\n",
    "        detech_length = 50\n",
    "        cold_period = 300\n",
    "        detect_version = (np.max(np.abs(freq))>0.01)     \n",
    "        if detect_version and (not self.detect_start):\n",
    "            self.detect_start=True\n",
    "        if self.detect_start:\n",
    "            if self.detect_step<detech_length:\n",
    "                p = self.collect(cost, freq)\n",
    "            self.detect_step += 1\n",
    "            if self.detect_step>detech_length:\n",
    "                self.current_controller_id = np.argmax(self.p)\n",
    "                if (np.max(np.abs(freq))>0.15):\n",
    "                    self.detect_step=0\n",
    "                    self.detect_start=False\n",
    "            if self.detect_step == (detech_length+cold_period):\n",
    "                self.detect_step=0\n",
    "                self.detect_start=False\n",
    "        return self.p\n",
    "    \n",
    "    def sample(self):\n",
    "        controller_idx = np.random.choice(self.controller_idx_set, p=self.p)\n",
    "        return controller_idx\n",
    "    \n",
    "    def collect(self, cost, freq):\n",
    "        self.batch_cost_list.append(cost.copy())\n",
    "        if len(self.batch_cost_list)==self.tau:\n",
    "            cost_batch = np.sum(self.batch_cost_list)            \n",
    "            self.old_controller_id = self.current_controller_id.copy()\n",
    "            new_p = self.update(cost_batch,self.current_controller_id)\n",
    "            self.batch_cost_list = []\n",
    "            self.freq_before_switch = freq.copy()\n",
    "            self.current_controller_id = self.sample()\n",
    "        else:\n",
    "            new_p = self.p\n",
    "        return new_p\n",
    "    \n",
    "    def get_action(self, x, action_s, env):\n",
    "        u, up, ui = Action(x, action_s, self.controller_set[self.current_controller_id],\\\n",
    "                            self.cell_set[self.current_controller_id], env)\n",
    "        return u, up, ui\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_loss_list = []\n",
    "freq_loss_list = []\n",
    "total_loss_list = []\n",
    "transient_action_cost_list = []\n",
    "transient_freq_cost_list = []\n",
    "transient_cost_list = []\n",
    "# controller_set = [layer_03, layer, layer_5]\n",
    "# cell_set = [cell_03, cell, cell_5]\n",
    "controller_set = [layer_05, layer_1, layer_03]\n",
    "cell_set = [cell_05, cell_1, cell_03]\n",
    "p_traj_list = []\n",
    "mode_predefined = [5.0,1.0,1.0,0.3,0.3]\n",
    "tau = 5\n",
    "for run_num in range(100):\n",
    "  switching_agent = exp3_ISS_algm(tau,controller_set,cell_set,eta=0.005)\n",
    "  p = switching_agent.current_p()\n",
    "  percentage_plot = np.random.choice([0.3,1.0,5.0], 1, p=[0.1, 0.45, 0.45])[0]\n",
    "  # percentage_plot=mode_predefined[0]\n",
    "  env = Frequency(Pm,M,M*percentage_plot,D,F,delta_t,max_action, dim_action,Penalty_action*3, coef_cost, incidence_comm,gamma=5)\n",
    "  Trajectory_RNN=[] \n",
    "  Record_u=[]\n",
    "  Record_up=[]\n",
    "  Record_ui=[]\n",
    "  record_freq = []\n",
    "  record_freq_norm = []\n",
    "  record_grad_ui = []\n",
    "  Trajectory_s = []\n",
    "  mode_traj = []\n",
    "  transient_list = []\n",
    "  transient_action_list = []\n",
    "  transient_start = False\n",
    "  p_traj = []\n",
    "  mode_traj.append(percentage_plot)\n",
    "\n",
    "  init_state=equilibrium_init\n",
    "\n",
    "  # init_state=env.reset()\n",
    "  x=init_state.copy().astype(np.float32)\n",
    "\n",
    "  env.set_state(x)\n",
    "  Trajectory_RNN.append(x)\n",
    "  SimulationLength=2000\n",
    "  Pm_init=Pm.copy()\n",
    "  # print(dist1.shape)\n",
    "  action_s = np.zeros((1,dim_action),dtype=np.float32)\n",
    "\n",
    "  Pm_change = Pm_init.copy() \n",
    "  for i in range(SimulationLength):\n",
    "      if i==10:\n",
    "        Pm_change=Pm_init.copy() + np.random.uniform(-1,1,((Pm_init).shape[1]))\n",
    "        transient_start = True\n",
    "        transient_time = 0\n",
    "      elif i == 700:\n",
    "        Pm_change=Pm_init.copy() + np.random.uniform(-1,1,((Pm_init).shape[1]))\n",
    "        transient_start = True\n",
    "        transient_time = 0\n",
    "\n",
    "      if (i+1)%500 == 0:\n",
    "        if percentage_plot == 0.3:\n",
    "            percentage_plot = np.random.choice([0.3,1.0], 1, p=[0.5, 0.5])[0]\n",
    "        elif percentage_plot == 1.0:\n",
    "            percentage_plot = np.random.choice([0.3,1.0,5.0], 1, p=[0.3, 0.4, 0.3])[0]\n",
    "        else:\n",
    "            percentage_plot = np.random.choice([1.0,5.0], 1, p=[0.5, 0.5])[0]\n",
    "        env.switch(percentage_plot)\n",
    "        # percentage_plot = 0.3\n",
    "        # percentage_plot= mode_predefined[(i+1)//500]\n",
    "        mode_traj.append(percentage_plot)\n",
    "\n",
    "      u, up, ui = switching_agent.get_action(x, action_s,env)\n",
    "\n",
    "      action_s, next_x, r= env.step_PI_CommEdge(up,ui,Pm_change)\n",
    "      \n",
    "      p_traj.append(p.copy())\n",
    "      # print(next_x.shape)\n",
    "      freq = np.dot(next_x, env.select_w)\n",
    "      # p=switching_agent.collect(r,freq)\n",
    "         \n",
    "      p = switching_agent.detect(r,freq,i)\n",
    "      \n",
    "      record_freq.append(freq)\n",
    "      if transient_start:\n",
    "        transient_list.append(freq)\n",
    "        transient_action_list.append(u)\n",
    "        transient_time += 1\n",
    "        if transient_time == 300:\n",
    "            transient_start=False\n",
    "      record_freq_norm.append(np.linalg.norm(freq))\n",
    "\n",
    "\n",
    "      x=next_x\n",
    "      Trajectory_RNN.append(x)\n",
    "      Record_u.append(np.squeeze(u))\n",
    "      Record_up.append(np.squeeze(up))\n",
    "      Record_ui.append(np.squeeze(ui))\n",
    "      \n",
    "      record_grad_ui.append(np.squeeze(env.calc_grad_action(ui)[1]))\n",
    "      Trajectory_s.append(np.squeeze(env.state_s))\n",
    "\n",
    "  record_freq = np.squeeze(np.asarray(record_freq))\n",
    "  action_array = np.asarray(Record_u)\n",
    "  transient_list = np.squeeze(transient_list)\n",
    "  p_traj_list.append(p_traj)\n",
    "\n",
    "\n",
    "  loss_action = 0.5*np.mean(np.sum(np.power(action_array,2)@env.diag_c,axis=1))\n",
    "  # loss_freq = np.sum(np.max(np.abs(record_freq)))/record_freq.shape[0] + 2*np.sum(np.abs(record_freq))/record_freq.shape[0]\n",
    "  loss_freq = 10*np.mean(np.linalg.norm(record_freq,axis=1))+ 10*np.mean(np.max(np.abs(record_freq),axis=1))\n",
    "  loss = 1*loss_action + 100*loss_freq\n",
    "  action_loss_list.append(loss_action)\n",
    "  freq_loss_list.append(100*loss_freq)\n",
    "  total_loss_list.append(loss)\n",
    "  \n",
    "  transient_freq = 10*np.mean(np.linalg.norm(np.squeeze(transient_list),axis=1)) + 10*np.mean(np.max(np.abs(transient_list),axis=1))\n",
    "  transient_act = np.squeeze(np.asarray(transient_action_list))\n",
    "  transient_act_cost = 0.5*np.mean(np.sum(np.power(transient_act,2)@env.diag_c,axis=1))\n",
    "  trans_cost = transient_act_cost + 100*transient_freq\n",
    "  transient_cost_list.append(trans_cost)\n",
    "  transient_action_cost_list.append(transient_act_cost)\n",
    "  transient_freq_cost_list.append(transient_freq*100)\n",
    "\n",
    "    # print(loss,loss_action,5*loss_freq)\n",
    "print('total cost:', np.mean(total_loss_list), np.std(total_loss_list))\n",
    "print('freq: ',np.mean(freq_loss_list),np.std(freq_loss_list))\n",
    "print('action: ',np.mean(action_loss_list),np.std(action_loss_list))\n",
    "\n",
    "print('***********************Transient**************************')\n",
    "print('transient total: ',np.mean(transient_cost_list),np.std(transient_cost_list))\n",
    "print('transient freq: ', np.mean(transient_freq_cost_list),np.std(transient_freq_cost_list))\n",
    "print('transient action: ', np.mean(transient_action_cost_list), np.std(transient_action_cost_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
